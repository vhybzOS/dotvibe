# Agent System - Hybrid Mastra + Google GenAI

A modular agent system that provides conversation management and LLM execution using functional programming patterns. Designed for seamless migration to full Mastra when Zod v4 support arrives.

## ðŸŽ¯ Architecture Overview

This system bridges the gap between mastra's conversation management patterns and Google GenAI execution, using battle-tested patterns from our production codebase.

```
src/agent/
â”œâ”€â”€ types.ts           # Core interfaces (ThreadContext, AgentConfig, etc.)
â”œâ”€â”€ models.ts          # Environment configuration and model mappings  
â”œâ”€â”€ token-tracking.ts  # Real-time token tracking with "240K/1M" display
â”œâ”€â”€ conversation.ts    # Mastra-compatible conversation management
â”œâ”€â”€ bridge.ts          # Hybrid orchestration (Mastra + Google GenAI)
â””â”€â”€ README.md         # This file
```

## ðŸ”„ Mastra Migration Strategy

### Current Implementation (Functional Patterns)

```typescript
import { createAgentBridge } from './src/agent/bridge.ts'

const config: AgentConfig = {
  model: 'gemini-2.5-flash',
  maxTokens: 1000000,
  apiKey: Deno.env.get('GOOGLE_API_KEY')!,
  enableFunctionCalling: true
}

const bridge = createAgentBridge(config, threadContext)
const response = await bridge.generateResponse('Hello, AI!')
```

### Future Migration (6-line Changes)

When Mastra supports Zod v4, replace with:

```typescript
import { Agent, GoogleAILLM } from '@mastra/core'

const agent = new Agent({
  llms: [new GoogleAILLM({ apiKey, model: 'gemini-2.5-flash' })],
  tools: [] // Import from @mastra/core/tools
})

const response = await agent.generate('Hello, AI!')
```

## ðŸ“‹ Migration Checklist

### Phase 1: Interface Compatibility âœ… COMPLETE
- [x] **Exact mastra MessageList interface** in `conversation.ts`
- [x] **Compatible conversation management** with method chaining
- [x] **Thread context tracking** with resourceId support
- [x] **Token tracking** with real-time progress display

### Phase 2: Google GenAI Integration âœ… COMPLETE  
- [x] **Battle-tested API patterns** from `tests/google-ai-integration.test.ts`
- [x] **Function calling support** with FunctionCallingConfigMode.ANY
- [x] **Error handling** with detailed AgentError types
- [x] **Token limit enforcement** with warnings

### Phase 3: Migration Readiness âœ… COMPLETE
- [x] **MASTRA MIGRATION comments** throughout codebase
- [x] **Drop-in replacement design** for agent.generate()
- [x] **Interface documentation** for seamless transition
- [x] **Test coverage** for all migration scenarios

## ðŸ”§ Core Components

### 1. AgentBridge (bridge.ts)

The main orchestration layer that combines conversation management with LLM execution:

```typescript
const bridge = createAgentBridge(config, threadContext)

// Add messages (supports chaining)
bridge
  .addMessage('User question', 'user')
  .addMessage('AI response', 'response')

// Generate responses
const response = await bridge.generateResponse('Hello!', {
  temperature: 0.7,
  enableFunctionCalling: true
})

// Track progress
const progress = bridge.getProgress() // "240K/1M (24%)"
```

### 2. ConversationManager (conversation.ts)

Mastra-compatible conversation management using functional patterns:

```typescript
const conversation = createConversationManager({
  threadId: 'thread-123',
  resourceId: 'resource-456'
})

// Exact mastra interface compatibility
conversation
  .add('Hello', 'user')
  .add('Hi there!', 'response')
  .addSystem('You are helpful')

const messages = conversation.get.all.v2()
const latest = conversation.getLatestUserContent()
```

### 3. TokenTracker (token-tracking.ts)

Real-time token tracking with human-readable display:

```typescript
const tracker = createTokenTracker(threadContext, (progress) => {
  console.log(`Thread: ${progress.current}/${progress.max}`)
})

tracker.addTokens({ totalTokens: 1500, inputTokens: 800, outputTokens: 700 })
const progress = tracker.getProgress() // { current: "1.5K", max: "1M", percentage: 0.15 }
```

## ðŸ§ª Testing Strategy

### Comprehensive Test Coverage âœ… 107 Tests Passing

- **bridge.test.ts**: Hybrid orchestration, Google GenAI integration
- **conversation.test.ts**: Mastra interface compatibility  
- **token-tracking.test.ts**: Progress tracking, formatting
- **models.test.ts**: Environment configuration, model mapping
- **types.test.ts**: Interface definitions, type safety

### Test Execution

```bash
# Run all agent tests
deno test tests/agent/ --allow-env --no-check

# Individual module tests
deno test tests/agent/bridge.test.ts --allow-env --no-check
deno test tests/agent/conversation.test.ts --no-check
```

## ðŸ”‘ Environment Configuration

### Required Environment Variables

```bash
# Google AI SDK
GOOGLE_API_KEY=your_api_key_here

# Model Configuration (optional - defaults provided)
GEMINI_CHAT_MODEL=gemini-2.5-flash
GEMINI_EMBEDDING_MODEL=text-embedding-004
```

### Configuration Loading

```typescript
import { loadAgentConfig } from './src/agent/models.ts'

const config = loadAgentConfig({
  // Override defaults
  model: 'gemini-2.5-flash',
  temperature: 0.7,
  verbose: true
})
```

## ðŸš€ Production Usage

### Basic Agent Setup

```typescript
import { createAgentBridge } from './src/agent/bridge.ts'
import { loadAgentConfig } from './src/agent/models.ts'

// Load configuration from environment
const config = loadAgentConfig()

// Create thread context
const threadContext = {
  threadId: crypto.randomUUID(),
  maxTokens: 1000000,
  currentTokens: 0,
  model: config.model
}

// Initialize bridge
const agent = createAgentBridge(config, threadContext)

// Generate response
const response = await agent.generateResponse('Analyze this code...', {
  temperature: 0.3,
  enableFunctionCalling: true,
  systemInstruction: 'You are an expert code reviewer.'
})

console.log(`Response: ${response.content}`)
console.log(`Tokens: ${response.tokenUsage.totalTokens}`)
console.log(`Progress: ${agent.getProgress().current}/${agent.getProgress().max}`)
```

### Advanced Usage with Function Calling

```typescript
// Enable function calling for tool usage
const response = await agent.generateResponse(
  'Search for authentication patterns in the codebase',
  {
    enableFunctionCalling: true,
    temperature: 0.0
  }
)

// Check for function calls
if (response.metadata?.functionCalls) {
  console.log('AI wants to call functions:', response.metadata.functionCalls)
}
```

## ðŸ—ï¸ Design Principles

### 1. Functional Programming (NO Classes)

All modules use higher-order functions and functional composition:

```typescript
// âŒ Don't use classes
export class AgentBridge { ... }

// âœ… Use functional patterns
export const createAgentBridge = (config, context) => {
  // State in closure
  const state = { config, context }
  
  // Return object with methods
  return {
    generateResponse: (prompt) => { ... },
    addMessage: (message) => { ... }
  }
}
```

### 2. Mastra Interface Compatibility

Every interface matches mastra patterns exactly:

```typescript
// Exact mastra MessageList interface
export interface ConversationManagerInstance {
  add(messages: string | MessageInput[], source: MessageSource): ConversationManagerInstance
  getLatestUserContent(): string | null
  get: {
    all: { v2(): MastraMessageV2[] }
    input: { v2(): MastraMessageV2[] }
    response: { v2(): MastraMessageV2[] }
  }
}
```

### 3. Battle-Tested Google GenAI Patterns

Uses exact patterns from working production code:

```typescript
// From tests/google-ai-integration.test.ts
const response = await genAI.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: prompt,
  config: {
    toolConfig: {
      functionCallingConfig: {
        mode: FunctionCallingConfigMode.ANY,
        allowedFunctionNames: ['tool_name']
      }
    },
    tools: [{ functionDeclarations }]
  }
})
```

## ðŸ”„ Migration Timeline

### Immediate Benefits âœ… Available Now
- Modular agent system with clean separation
- Real-time token tracking with progress display
- Battle-tested Google GenAI integration
- Comprehensive test coverage (107 tests)

### Future Migration (When Mastra Supports Zod v4)
- Replace entire bridge.ts with single Agent import
- Swap conversation.ts for mastra MessageList
- Update tool definitions to @mastra/core/tools
- **Total changes**: ~6 lines across 3 files

## ðŸ“Š Performance Metrics

### Token Tracking Accuracy
- Real-time progress updates: "240K/1M (24%)"
- Token estimation with 4 chars/token heuristic
- Input/output token separation
- Context window limit enforcement

### Conversation Management
- Message deduplication for system messages
- Thread-aware persistence with resourceId
- Multiple format support (v1, v2, UI, core)
- Method chaining for fluent interface

### Error Handling
- Detailed AgentError types with metadata
- Graceful degradation for API failures
- Token limit warnings and enforcement
- Comprehensive validation with Zod v4

---

This agent system provides immediate production value while maintaining a clear path to full Mastra integration. The functional architecture ensures maintainability and the comprehensive test suite guarantees reliability.